# Memory Log - 2026-02-14

### [00:05] Learnings
**Learning (Technical):** When implementing FTS5 triggers for SQLite, triggers cannot call Python code. The text column must be populated separately via Python after insert/reindex. Solution: call rebuild_fts() after reindex() at startup to populate semantic text.

### [00:05] Learnings
**Learning (Technical):** sqlite-vec vec0 tables use KNN query pattern: WHERE embedding MATCH ? AND k = ? for O(log n) performance. Do not use brute force vec_distance_cosine() which is O(n).

### [00:05] Learnings
**Learning (Technical):** Threading.Lock in Python is in-process only - does not protect against multi-process race conditions. For cross-process safety, use SQLite transactions with BEGIN IMMEDIATE, or external locking (portalocker library on Windows).

### [00:05] Learnings
**Learning (Process):** When documenting limitations, include: Symptoms (how it fails), When it triggers (specific conditions), Severity, Detection method, Mitigations ranked by effort/safety, and explicit Non-goals. This makes the doc actionable rather than placeholder-y.

### [00:05] Task Completed
**Task:** Implement Phase 1B semantic search with FTS5 and graceful degradation
**Outcome:** Complete. FTS5 full-text search working with BM25 scoring. vec0 KNN pattern ready for sqlite-vec. Hybrid search degrades gracefully to FTS-only mode. 172 artifacts indexed with semantic text.

### [00:05] Task Completed
**Task:** Upgrade health check with FTS completeness and embedding coverage metrics
**Outcome:** Complete. Health check now reports: fts_text_missing_count, coverage_pct, vec_table_exists, embeddings_count vs artifacts_count, first_broken_entry for audit chain. Overall status correctly promotes errors.

### [00:05] Task Completed
**Task:** Create migration system with schema_migrations tracking
**Outcome:** Complete. migrations/runner.py with checksum verification, m001_add_vectors.py for FTS5+vec0 tables. Migrations are idempotent and tracked.

### [00:06] Task Completed
**Task:** Document multi-process audit chain limitation
**Outcome:** Complete. KNOWN_LIMITATIONS.md created with proper structure: symptoms, triggers, severity, detection, mitigations ranked by effort/safety, and explicit non-goals.

### [00:06] Task Completed
**Task:** Write Phase 1B integration tests
**Outcome:** Complete. tests/test_phase1b_integration.py with golden-path (store->FTS populated) and failure-path (FTS fail->save succeeds) tests. All 8 tests pass.

### [00:45] Learnings
**Learning (Technical):** When implementing temporal queries with text-based ISO timestamp comparisons in SQLite, always normalize input timestamps to a consistent format (Z-suffix) before comparison. Mixed formats (+00:00 vs Z) can cause subtle comparison bugs even though they represent the same time.

### [00:45] Learnings
**Learning (Technical):** For conditional expensive operations (like FTS rebuild), check if the work is actually needed before doing it. In reindex(), checking missing_text_count > 0 before calling rebuild_fts() avoids O(n) overhead when FTS is already complete.

### [00:45] Learnings
**Learning (Process):** When consolidating duplicate code into a shared utility (like time_utils.py), the migration path is: 1) Create utility, 2) Update imports across files, 3) Fix any semantic bugs discovered during consolidation (like timezone mismatch in ranking_config.py).

### [00:45] Task Completed
**Task:** Cleanup sprint: Centralized datetime utils, fixed timezone mismatch, consolidated migration tables, made FTS rebuild conditional
**Outcome:** Created time_utils.py with utc_now(), utc_now_iso(), parse_iso_datetime(), days_since(), normalize_iso_z(). Updated 8 files. Fixed ranking_config.py aware vs naive datetime bug. Migration tracking now single table (schema_migrations).

### [00:45] Task Completed
**Task:** Phase 2: Temporal fields and fact supersession
**Outcome:** Added valid_from, valid_until, superseded_by, importance, pinned to fact schema. Created m002_add_temporal migration. Implemented supersede_fact(), add_relation(), get_relations(), query_current_facts(). Added MCP tools duro_supersede_fact and duro_get_related. All tests pass.

### [01:13] Learnings
**Learning (Technical):** When adding new SQLite columns via migrations, the smoke test/pre-push hook will fail if the migration hasn't been applied to the actual database being tested. Fix: auto-apply migrations in smoke_test.py before running tests, or fail fast with a clear "run migration X" message.

### [01:13] Learnings
**Learning (Process):** Always log the database path at the start of test runners. Having two plausible DB paths on disk (e.g., ~/.duro/index.db vs ~/.agent/memory/index.db) causes confusion. Explicit logging prevents wrong-DB debugging sessions.

### [01:13] Learnings
**Learning (Process):** Pre-push hooks that run smoke tests are valuable - they caught the migration drift before it hit CI. The error was noisy (500 "no such column" lines) but the failure was correct. Improving the error message (auto-migrate or fail fast) makes the hook more developer-friendly.

### [01:13] Learnings
**Learning (Technical):** For pytest discovery, put tests in a tests/ folder with conftest.py that adds the parent directory to sys.path. This is cleaner than having test files in the project root.

### [01:13] Task Completed
**Task:** Phase 4: Confidence Decay & Maintenance implementation
**Outcome:** Completed. Added decay.py with explicit math, migration m003_add_reinforcement, 4 MCP tools (duro_apply_decay, duro_reembed, duro_maintenance_report, duro_reinforce_fact), and golden decay test (5/5 passing). Committed fba51ff.

### [01:13] Task Completed
**Task:** Fix migration drift in pre-push hook
**Outcome:** Push initially failed because m003 migration wasn't applied to ~/.agent/memory/index.db. Applied migration manually, push succeeded. Then improved smoke_test.py to auto-apply migrations.

### [01:13] Task Completed
**Task:** Improve test infrastructure robustness
**Outcome:** Added DB path logging to smoke_test.py, auto-migration detection/apply before tests, moved tests to tests/ folder with proper pytest structure. Committed 746d1bc.

### [01:35] Learnings
**Learning (Process):** When writing error messages that tell users to run a command, make the command exactly copy-pasteable. Include all required arguments and subcommands (e.g., include "up" in migration runner commands).

### [01:35] Learnings
**Learning (Technical):** Use precise glob patterns for migrations: m[0-9][0-9][0-9]_*.py instead of m*.py. Prevents helper modules from being counted as migrations.

### [01:35] Task Completed
**Task:** Polish test infrastructure with stable imports and env var control
**Outcome:** Added exports to migrations/__init__.py, DURO_SMOKE_APPLY_MIGRATIONS env var (=1 auto-apply, =0 strict), migrations dir logging. Commits d72a4b9 and 4b66563.

### [02:24] Task Completed
**Task:** Implement Decision Outcomes v1 - wire decisions into feedback loop
**Outcome:** Complete. Added decisions_used to episodes, auto-generate decision updates in store_evaluation with de-dupe, deterministic status rules (validated >= 0.7, reversed <= 0.3), guardrails for decision updates. All 6 smoke tests pass.

### [02:24] Learnings
**Learning (Technical):** Decision outcomes pattern: decisions_used + decisions_created de-duped into single set, smaller deltas than skills (+0.005 success, +0.002 partial, -0.01 failed), deterministic status thresholds (0.7 validated, 0.3 reversed). Decisions are hypotheses that survive contact with reality.

### [02:29] Learnings
**Learning (Technical):** When implementing compounding feedback loops: 1) Use explicit `is not None` checks for deltas to avoid footguns if partial=0.0 later. 2) Separate `verified_at` (status became validated) from `last_evaluated_at` (any confidence change) for cleaner audit trails. 3) Set delta sign explicitly (-0.01) rather than using negation logic to avoid comment/code mismatch bugs.

### [02:29] Learnings
**Learning (Process):** Code review catches that prevent future haunting: 1) Comments that say "negative" next to positive values will cause someone to "fix" what isn't broken. 2) Truthy checks on numeric values (`if delta`) are footguns - use explicit None checks. 3) Semantic field names matter - "verified_at" shouldn't update when status stays "unverified".

### [02:29] Learnings
**Learning (Technical):** Any compounding loop without an opposing force will saturate at 1.0. Staleness decay (tiny penalty if `now - last_used_at > N days`) is the antidote. Track `last_used_at` on every decision update for v1.1.

### [02:29] Task Completed
**Task:** Session: Decision Outcomes v1 implementation + code review fixes
**Outcome:** Complete. Implemented decisions_used in episodes, auto-generate decision updates in evaluations with de-dupe, deterministic status rules (validated >= 0.7, reversed <= 0.3). Applied 5 code review fixes: explicit None checks, cleaner audit trail (verified_at vs last_evaluated_at), removed negation logic, tighter test tolerance. All 6 smoke tests pass. Pushed as commit 9fd9a15.

### [02:58] Learnings
**Learning (Technical):** Lean context loading pattern: Extract only task completions from today's log (skip learnings/session noise), trim core memory to specific sections (User Preferences, Important Context), include only recently-validated decisions (updated within 7 days, confidence >= 0.5), and cap yesterday's summary. Result: 77% token reduction (7K → 1.6K tokens) while preserving actionable context.

### [02:58] Learnings
**Learning (Process):** When implementing context loading modes, keep default as existing behavior for backwards compatibility. New callers (CLAUDE.md) explicitly opt into optimized mode. This prevents silent breaking changes while enabling improvements.

### [02:58] Learnings
**Learning (Technical):** For artifact queries with optional nested fields (like decision.outcome.confidence), always check if parent object is None before calling .get() on it. Pattern: `if outcome is None or isinstance(outcome, str): continue`

### [02:58] Task Completed
**Task:** Implement Lean Context v1 - three-mode context loading for duro_load_context
**Outcome:** Complete. Added full/lean/minimal modes. Lean mode: 77% token reduction (28K→6.5K chars). New helpers: load_today_tasks_only(), load_core_trimmed(), load_recent_summary(), get_active_decisions(). Updated CLAUDE.md to call mode="lean". All smoke tests pass. Pushed as commits 577cc0c (duro-mcp) and a132863 (.claude).

### [03:28] Learnings
**Learning (Architecture):** Cartridge/modular memory research: UC Berkeley's MemGPT (arXiv:2310.08560) is the formal foundation. Key insight from Letta benchmarks: simple filesystem operations (74% accuracy) beat specialized memory tools (68.5%). Simpler tools agents have seen in training work better than exotic memory systems. For Duro, task-aware loading with existing tools likely beats building complex virtual memory manager.

### [03:55] Learnings
**Learning (Tools):** Planify has an alternative config (planify-openai.yaml) that uses OpenAI for all agents when Anthropic API key isn't available. The default config requires both keys but the OpenAI-only variant works as a fallback.

### [03:55] Task Completed
**Task:** Run Planify on stride-server reward system feature
**Outcome:** Generated comprehensive implementation plan including Prisma schema, 5 API endpoints, 4 UI components, risks, and 23 tasks. Cost: $0.14. Used OpenAI-only config as workaround for missing Anthropic key.

### [03:55] Task Completed
**Task:** Research and analyze cartridge memory concept
**Outcome:** Completed comprehensive analysis. Formal name: Virtual Context Management (MemGPT/UC Berkeley). Key finding: simple tools beat complex architectures (Letta: 74% vs 68.5%). Banked as idea/concept for future consideration.
