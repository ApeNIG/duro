# Memory Log - 2026-02-18

### [01:02] Learnings
**Learning (Process):** When building UI for a product, always create visual mockups FIRST using design tools (like Pencil MCP) before implementing code. This allows for faster iteration on layout/design decisions without the overhead of code changes, better stakeholder alignment before committing development time, and clearer visual direction that can be referenced during implementation.

### [01:02] Task Completed
**Task:** Built complete Next.js 14 Studio Web UI for Olootu BYOC Template Platform
**Outcome:** Successfully created 64 files (11,635 lines) including: TypeScript types, API client, layout components (Navbar, Sidebar, Footer), Landing page with hero/features/CTA, Templates gallery with mock data fallback, Template detail page with dynamic form generation (VariableInput supporting text/number/select/image/color/boolean), TemplateRunner with execution flow, Providers page with status and setup guides, Settings page for API key management. Build passes successfully. Pushed to GitHub commit c9cd170.

### [01:02] Task Completed
**Task:** Started creating UI mockups for Olootu Studio using Pencil design tools
**Outcome:** In progress - retrieved Swiss Clean style guide and design system guidelines. Ready to create mockup frames for Landing page, Templates gallery, and Template detail screens. Session ended before completing mockups.

### [01:50] Task Completed
**Task:** Set up Duro eval tracking system
**Outcome:** Created eval_metrics skill (ops/eval_metrics.py) that computes decision review rate, unverified rate, reopen rate. Registered in skills index. Created weekly_eval_metrics checklist. Validated 3 Phase 1 Feedback Loop decisions. Current baseline: 60 real decisions, 50 unverified (after validations), 9 validated, 0% reopen rate.

### [01:59] Task Completed
**Task:** Clear decision review backlog
**Outcome:** Validated 44 decisions in bulk (40 validated, 4 superseded). Unverified rate dropped from 88% to 15%. Only 9 decisions remain needing review. Decision feedback loop now closed.

### [02:02] Learnings
**Learning (Process):** Bulk decision validation is high-leverage: clearing 44 decisions in one session dropped unverified rate from 88% to 15%. Batch reviewing by category (architecture, design, skills, roadmap) is more efficient than one-by-one.

### [02:02] Learnings
**Learning (Technical):** Eval tracking should be Phase 0 in any governance roadmap. Build the scoreboard before building the governance - you can't improve what you don't measure.

### [02:02] Learnings
**Learning (Technical):** When building Python skills that access ArtifactStore, use store.store_fact() not store.store_artifact(). The API is method-per-type (store_fact, store_decision, store_episode), not generic.

### [02:02] Learnings
**Learning (Process):** Critical analysis of governance roadmaps: distinguish between incident-driven improvements (needed now) vs speculative improvements (build when pain appears). The right question is "what's the incident list that motivates this?"

### [02:02] Task Completed
**Task:** Session: Duro upgrade planning and eval tracking setup
**Outcome:** 1) Critically analyzed governance roadmap - identified incident-driven vs speculative improvements. 2) Built eval_metrics skill tracking decision review rate, unverified rate, reopen rate. 3) Created weekly_eval_metrics checklist. 4) Cleared decision backlog: 44 decisions reviewed, unverified rate 88%â†’15%. 5) Stored 3 metrics snapshots as facts for trend tracking.

### [02:37] Learnings
**Learning (Technical):** code_review_verifier skill works well for Python AST analysis but has false positives: flagged TemplateCompiler.compile() as banned function because it matches the pattern for Python's built-in compile(). Need to improve detection to distinguish method calls from built-in calls.

### [02:37] Learnings
**Learning (Technical):** test_generate skill produces useful test stubs but generated tests need manual refinement: imports assume modules are available (need try/except guards), async tests need pytest-asyncio which may not be installed, mock classes may miss abstract methods from base classes.

### [02:37] Learnings
**Learning (Process):** Pipeline validation on real project (olootu) revealed test coverage gap: 126 Python files with 427 testable units but only 1 test file. test_generate can bootstrap coverage but requires dependency installation (poetry add pytest pytest-asyncio pytest-cov) to run generated tests.

### [02:37] Learnings
**Learning (Process):** Code pipeline execution order matters: run code_review first to identify hotspots, then test_coverage to find gaps, then test_generate for untested modules. Don't run code_refactor until tests exist to verify behavior preservation.

### [02:37] Task Completed
**Task:** Run code dev pipeline on olootu project
**Outcome:** Successfully validated code_review_verifier and test_generate skills. Generated 110 test stubs covering 5 priority modules. Identified skill improvements needed: false positive detection in code_review, dependency handling in test_generate.

### [02:41] Task Completed
**Task:** Fix code_review_verifier false positive for method calls
**Outcome:** Distinguished ast.Name calls (built-ins like eval) from ast.Attribute calls (methods like obj.compile). Added DANGEROUS_ATTR_CALLS set for explicit module.function patterns. Verified fix catches real threats while ignoring safe method calls.

### [02:47] Task Completed
**Task:** Fix test_generate skill for async test handling
**Outcome:** Fixed import order (pytest before skip_async), added @skip_async and @pytest.mark.asyncio decorators to all async test generation methods, added try/except guards around module imports. Commit 83e0d9c pushed to duro repo.
