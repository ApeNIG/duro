# Memory Log - 2026-02-18

### [01:02] Learnings
**Learning (Process):** When building UI for a product, always create visual mockups FIRST using design tools (like Pencil MCP) before implementing code. This allows for faster iteration on layout/design decisions without the overhead of code changes, better stakeholder alignment before committing development time, and clearer visual direction that can be referenced during implementation.

### [01:02] Task Completed
**Task:** Built complete Next.js 14 Studio Web UI for Olootu BYOC Template Platform
**Outcome:** Successfully created 64 files (11,635 lines) including: TypeScript types, API client, layout components (Navbar, Sidebar, Footer), Landing page with hero/features/CTA, Templates gallery with mock data fallback, Template detail page with dynamic form generation (VariableInput supporting text/number/select/image/color/boolean), TemplateRunner with execution flow, Providers page with status and setup guides, Settings page for API key management. Build passes successfully. Pushed to GitHub commit c9cd170.

### [01:02] Task Completed
**Task:** Started creating UI mockups for Olootu Studio using Pencil design tools
**Outcome:** In progress - retrieved Swiss Clean style guide and design system guidelines. Ready to create mockup frames for Landing page, Templates gallery, and Template detail screens. Session ended before completing mockups.

### [01:50] Task Completed
**Task:** Set up Duro eval tracking system
**Outcome:** Created eval_metrics skill (ops/eval_metrics.py) that computes decision review rate, unverified rate, reopen rate. Registered in skills index. Created weekly_eval_metrics checklist. Validated 3 Phase 1 Feedback Loop decisions. Current baseline: 60 real decisions, 50 unverified (after validations), 9 validated, 0% reopen rate.

### [01:59] Task Completed
**Task:** Clear decision review backlog
**Outcome:** Validated 44 decisions in bulk (40 validated, 4 superseded). Unverified rate dropped from 88% to 15%. Only 9 decisions remain needing review. Decision feedback loop now closed.

### [02:02] Learnings
**Learning (Process):** Bulk decision validation is high-leverage: clearing 44 decisions in one session dropped unverified rate from 88% to 15%. Batch reviewing by category (architecture, design, skills, roadmap) is more efficient than one-by-one.

### [02:02] Learnings
**Learning (Technical):** Eval tracking should be Phase 0 in any governance roadmap. Build the scoreboard before building the governance - you can't improve what you don't measure.

### [02:02] Learnings
**Learning (Technical):** When building Python skills that access ArtifactStore, use store.store_fact() not store.store_artifact(). The API is method-per-type (store_fact, store_decision, store_episode), not generic.

### [02:02] Learnings
**Learning (Process):** Critical analysis of governance roadmaps: distinguish between incident-driven improvements (needed now) vs speculative improvements (build when pain appears). The right question is "what's the incident list that motivates this?"

### [02:02] Task Completed
**Task:** Session: Duro upgrade planning and eval tracking setup
**Outcome:** 1) Critically analyzed governance roadmap - identified incident-driven vs speculative improvements. 2) Built eval_metrics skill tracking decision review rate, unverified rate, reopen rate. 3) Created weekly_eval_metrics checklist. 4) Cleared decision backlog: 44 decisions reviewed, unverified rate 88%â†’15%. 5) Stored 3 metrics snapshots as facts for trend tracking.

### [02:37] Learnings
**Learning (Technical):** code_review_verifier skill works well for Python AST analysis but has false positives: flagged TemplateCompiler.compile() as banned function because it matches the pattern for Python's built-in compile(). Need to improve detection to distinguish method calls from built-in calls.

### [02:37] Learnings
**Learning (Technical):** test_generate skill produces useful test stubs but generated tests need manual refinement: imports assume modules are available (need try/except guards), async tests need pytest-asyncio which may not be installed, mock classes may miss abstract methods from base classes.

### [02:37] Learnings
**Learning (Process):** Pipeline validation on real project (olootu) revealed test coverage gap: 126 Python files with 427 testable units but only 1 test file. test_generate can bootstrap coverage but requires dependency installation (poetry add pytest pytest-asyncio pytest-cov) to run generated tests.

### [02:37] Learnings
**Learning (Process):** Code pipeline execution order matters: run code_review first to identify hotspots, then test_coverage to find gaps, then test_generate for untested modules. Don't run code_refactor until tests exist to verify behavior preservation.

### [02:37] Task Completed
**Task:** Run code dev pipeline on olootu project
**Outcome:** Successfully validated code_review_verifier and test_generate skills. Generated 110 test stubs covering 5 priority modules. Identified skill improvements needed: false positive detection in code_review, dependency handling in test_generate.

### [02:41] Task Completed
**Task:** Fix code_review_verifier false positive for method calls
**Outcome:** Distinguished ast.Name calls (built-ins like eval) from ast.Attribute calls (methods like obj.compile). Added DANGEROUS_ATTR_CALLS set for explicit module.function patterns. Verified fix catches real threats while ignoring safe method calls.

### [02:47] Task Completed
**Task:** Fix test_generate skill for async test handling
**Outcome:** Fixed import order (pytest before skip_async), added @skip_async and @pytest.mark.asyncio decorators to all async test generation methods, added try/except guards around module imports. Commit 83e0d9c pushed to duro repo.

### [02:53] Task Completed
**Task:** Create code_pipeline orchestrator skill
**Outcome:** Created skill that runs code_review, test_coverage, and test_generate stages in sequence. Features: health score (0-100), unified report, recommendations, CI exit codes. Tested on olootu: 171 files, 275 findings, 8 tests generated. Commit a2e543a.

### [02:57] Task Completed
**Task:** Validate code_refactor skill on olootu
**Outcome:** Validated 8 refactoring operations on 119 Python files. Results: remove_unused_imports (100% success, 6/20 changes), sort_imports (100% success, 15/20 changes), rename_* (working), extract_function (working). Bug found: remove_dead_code incorrectly flagged JSONResponse return content. 3 operations not yet implemented (add_type_hints, extract_variable, inline_variable).

### [02:59] Task Completed
**Task:** Fix remove_dead_code false positive bug
**Outcome:** Rewrote remove_dead_code to use AST analysis instead of line-by-line parsing. Now correctly handles multi-line return/raise statements. Tested on 20 olootu files with 0 false positives. Commit dadcc6f.

### [03:03] Task Completed
**Task:** Validate test_coverage_verifier skill
**Outcome:** All features validated with synthetic test data. 3 format parsers working (Cobertura XML, JSON, LCOV). Features tested: threshold enforcement, uncovered file reporting, regression detection, exclusion patterns. Skill is production-ready.

### [03:14] Learnings
**Learning (Technical):** When integrating skills that return results, always verify the exact key names in the return dict. The code_pipeline fix stage was checking for "changes_count" but code_refactor returns "changes" (a list). Use len(result.get("changes", [])) instead of result.get("changes_count", 0).

### [03:14] Learnings
**Learning (Technical):** The remove_unused_imports function in code_refactor.py has a bug: it removes entire import lines rather than just unused names. If "from typing import List, Any" has Any unused but List used, the whole line gets removed. This needs AST-aware line rewriting to fix properly.

### [03:14] Task Completed
**Task:** Debug and fix --fix mode in code_pipeline showing 0 changes
**Outcome:** Fixed bug where run_fix_stage was checking result.get("changes_count") but code_refactor returns "changes" (list). Changed to len(result.get("changes", [])). Verified: 5 files fixed with 8 changes on olootu. Committed 178ee30, pushed to origin.

### [03:22] Learnings
**Learning (Technical):** Multi-line imports with parentheses (from X import (\n    A,\n    B,\n)) require special handling in refactoring tools. Both remove_unused_imports and sort_imports had critical bugs that corrupted code by not properly handling the full line range (node.lineno to node.end_lineno). Must test refactoring tools on codebases with multi-line imports before enabling auto-fix.

### [03:27] Learnings
**Learning (Process):** Fix-chasing antipattern: When a fix reveals another bug, which reveals another, STOP. Disable the dangerous code path entirely and revisit later with proper test cases. Don't keep "fixing" in a live session - each iteration risks making things worse.

### [03:27] Learnings
**Learning (Technical):** AST-based code refactoring requires isolated development with comprehensive test cases covering: single-line imports, multi-line parenthesized imports, imports with comments, imports with aliases, mixed import styles. Never test refactoring tools on real codebases first.

### [03:27] Task Completed
**Task:** Investigate and fix --fix mode in code_pipeline
**Outcome:** Found initial bug (changes_count vs changes), fixed it. But discovered deeper bugs in remove_unused_imports and sort_imports that corrupt multi-line imports. After multiple failed fix attempts that made things worse, disabled fix operations entirely. Incident logged: inc_20260218_032223_cgiq4t. Committed 49e69d8.

### [03:31] Open Issues
**ISSUE:** Olootu UI design (black/yellow polished look) was lost/reverted to basic styling. Need to investigate and restore in future session. Possibly related to the --fix mode incident or an untracked file issue.

### [03:49] Learnings
**Learning (Process):** Skill validation pattern: Read full skill file, trace through main logic paths, check SKILL_META structure, verify run() signature, look for edge cases in parsing/processing code. For tested tier promotion, add "validated" date field.

### [03:49] Learnings
**Learning (Technical):** design_to_code_verifier uses "honest mode" - confidence levels (0.95 arbitrary, 0.90 css_var, 0.30 best_effort, 0.0 unresolved) instead of guessing unknown Tailwind colors. Reports "unresolved" rather than pretending to know.

### [03:49] Learnings
**Learning (Technical):** code_scaffold uses atomic directory creation - creates in temp dir first, validates (Python syntax, JSON syntax), then moves to final location. Cleanup on failure prevents partial scaffolds.

### [03:49] Task Completed
**Task:** Validate adversarial_planning skill
**Outcome:** Promoted core -> tested. 3-phase planning with Architect/Critic/Integrator pattern, proper go/no-go calculation based on critique severity.

### [03:49] Task Completed
**Task:** Validate design_to_code_verifier skill
**Outcome:** Promoted core -> tested (v2.0.1). Tailwind class extraction, CSS variable resolution, confidence-based matching, devkit JSON output for CI/CD.

### [03:49] Task Completed
**Task:** Validate code_scaffold skill
**Outcome:** Added validated date (v1.0.1). Already at tested tier. 4 templates (python-package, react-app, express-api, cli-tool), atomic creation, post-gen validation.

### [05:23] Learnings
**Learning (Technical):** Artifact linkage (artifact_type + artifact_id) on PendingReward enables deterministic reopen matching. Without it, cancel_pending_reward relies on action_id which can be ambiguous when multiple rewards share the same action pattern. Store the artifact identity at reward creation time so reopens can precisely target the correct pending reward.

### [05:23] Learnings
**Learning (Technical):** When implementing fail-closed policies, compute the risk classification FIRST (outside the try block). If risk classification itself fails, you can't know whether to fail-open or fail-closed - so default to fail-closed. This prevents the "can't classify so I'll assume safe" vulnerability.

### [05:23] Learnings
**Learning (Technical):** Never rely on string matching in security-critical code paths. A wording change becomes a security bug. Use structured fields (like allowed_via_token: bool) instead of parsing reason strings. The field is explicit, typed, and won't break when someone edits a message.

### [05:23] Task Completed
**Task:** Add artifact linkage to PendingReward for deterministic reopen matching
**Outcome:** Added artifact_type and artifact_id fields to PendingReward, updated record_provisional_success, cancel_pending_reward with match priority (artifact linkage > action_id > generic), updated handle_reopen_event with match_method tracking, added persistence, 4 new tests (28 total). Pushed to .agent repo.

### [05:23] Task Completed
**Task:** Fix gate_tool to use computed risk enum and structured allowed_via_token
**Outcome:** Computed risk FIRST outside try block (fail-closed if classification fails), used is_risky = risk in (DESTRUCTIVE, CRITICAL) instead of is_destructive flag, used result.allowed_via_token structured field instead of string matching reason. Pushed to duro-mcp repo.

### [06:18] Learnings
**Learning (Memory Hygiene):** Test artifacts polluting memory: 310 smoke-test facts (69% of all facts) were cleaned on Feb 18. Prevention: test-tagged facts need auto-expiry TTL. Always check fact quality, not just count.

### [06:42] Task Completed
**Task:** Memory landfill cleanup and decision-evidence linkage
**Outcome:** Deleted 627+ smoke-test artifacts (310 facts, 117 decisions, 200+ episodes). Reduced total from 2441 to 1657. Linked 6 decisions to real evidence episodes. Created cleanup episode ep_20260218_064207_2xq2ei.

### [06:43] Learnings
**Learning (Technical):** Smoke tests that create artifacts must use isolated test databases or cleanup hooks - production memory pollution from 200+ test artifacts required manual cleanup session

### [06:43] Learnings
**Learning (Process):** Decision-evidence linkage requires real work episodes - smoke-test episodes provide zero evidence value. Link decisions to episodes where outcomes are actually observed.

### [06:43] Learnings
**Learning (Technical):** Memory landfill cleanup pattern: query by tag (smoke-test), batch delete with reason, prune orphan embeddings, verify with status check. All deletions auto-backup to .agent/memory/backups/

### [06:43] Task Completed
**Task:** Session: Memory landfill cleanup + decision-evidence linkage
**Outcome:** Completed across 2 sessions. Removed 627+ smoke-test artifacts. Established decision-episode linking pattern. Documented as incident inc_20260218_061853_bjqblk and episode ep_20260218_064207_2xq2ei. Memory system now clean with 1657 real artifacts.

### [08:06] Task Completed
**Task:** Implement governance audit breadcrumb (autonomy_block_events)
**Outcome:** Added append-only autonomy_block_events field to RunLog. Records tool blocks at 4 points with {tool_name, domain, risk, reason, timestamp}. 7 invariant tests passing. Commit cce9a16.

### [08:23] Learnings
**Learning (Technical):** MCP tools that bypass orchestrator don't go through autonomy gates. Direct MCP tool calls must have their own gate logic that: (1) classifies action domain, (2) builds canonical action_id as f"{tool_name}_{domain}", (3) prechecks permission, (4) consumes token if allowed_via_token. Without this, approval tokens aren't consumed and one-shot property fails.

### [08:23] Learnings
**Learning (Technical):** Action ID binding is critical for token governance. Using wrong domain suffix (e.g., _knowledge vs _general) causes token to not match the gate check. Standardize with single make_action_id(tool, domain) function used everywhere.

### [08:23] Learnings
**Learning (Process):** "Read-only checks" means zero writes - not just "no file edits". Creating episodes, recording outcomes, logging tasks are all writes. When user says read-only, use only read/search/classify tools.

### [08:23] Learnings
**Learning (Process):** Evidence quality: "tool outputs" proves "dashboard lights on" not "engine bolted to chassis". Real evidence = file paths + function names + code excerpts. Show where in code, not just what API returns.

### [08:23] Task Completed
**Task:** Level 1: Memory-First Sanity Pass (redo with correct constraints)
**Outcome:** Clean pass. Zero writes, code-level evidence only. Cited orchestrator.py:211-214 (__init__ heartbeat), :944-949 (_finalize_run heartbeat), :242-244 (_gate_tool), :291-294 (token consumption). Proved architecture exists, not just tool responses.

### [08:23] Task Completed
**Task:** Level 2: Governance audit breadcrumb feature
**Outcome:** Added autonomy_block_events field to RunLog. 4 recording points in _gate_tool. 7 tests (all passing). Commit cce9a16. 276 insertions.

### [08:23] Task Completed
**Task:** Level 3: Adversarial Autonomy Drill
**Outcome:** Exposed 2 bugs: (1) action_id domain mismatch (_knowledge vs _general), (2) MCP tools bypass token consumption. Token binding works when IDs match. Mini-redo proved can_execute flips with correct action_id.

### [08:23] Task Completed
**Task:** Fix MCP token bypass vulnerability
**Outcome:** Added autonomy gate to duro_delete_artifact in duro_mcp_server.py:4252-4280. Precheck + token consumption before delete. Commit 99c83d4. 30 lines added. Requires server restart.

### [08:28] Task Completed
**Task:** Level 3 Adversarial Autonomy Drill (session 2)
**Outcome:** All governance gates validated: (A) Safe action allowed via L2, (B) Destructive action blocked without approval, (C) One-shot token consumed and verified non-reusable. Audit chain intact (800 entries). Token action_id matching enforced correctly. Reputation updated.

### [08:31] Learnings
**Learning (Governance):** Approval token action_id must match EXACTLY what the tool's gate expects. The delete_artifact gate looks for `duro_delete_artifact_{domain}` format, not arbitrary IDs. This strict matching is a security feature that prevents token misuse across different action types.

### [08:31] Learnings
**Learning (Technical):** The `can_execute` precheck doesn't verify active approval tokens - it only checks base permission level. The actual tool gate (`duro_delete_artifact`) is what consumes and validates tokens. This is correct design: precheck gives fast feedback, actual gate enforces security.

### [08:31] Learnings
**Learning (Process):** Adversarial autonomy drills validate three critical paths: (1) safe actions allowed, (2) risky actions blocked without approval, (3) one-shot tokens consumed and non-reusable. All three must pass for governance to be considered working.
