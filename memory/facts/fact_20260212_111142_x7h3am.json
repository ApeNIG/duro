{
  "id": "fact_20260212_111142_x7h3am",
  "type": "fact",
  "version": "1.1",
  "created_at": "2026-02-12T11:11:42.843036Z",
  "updated_at": null,
  "sensitivity": "public",
  "tags": [
    "AI-evaluation",
    "METR",
    "benchmarks",
    "safety-testing"
  ],
  "source": {
    "workflow": "manual",
    "run_id": null,
    "tool_trace_path": null
  },
  "data": {
    "claim": "METR's key evaluation frameworks: RE-Bench (AI vs human experts on ML tasks), Monitorability Evals (detecting AI bypassing oversight), MALT Dataset (cataloging bad behaviors like reward hacking, sandbagging, deception), Autonomy Evaluations (self-improvement, rogue replication, sabotage potential).",
    "source_urls": [
      "https://metr.org/research",
      "https://metr.org/blog/2024-03-13-autonomy-evaluation-resources/"
    ],
    "snippet": null,
    "confidence": 0.9,
    "verified": true,
    "evidence_type": "paraphrase",
    "provenance": "web"
  }
}