# Summary: 2026-02-18

## Learnings
- When building UI for a product, always create visual mockups FIRST using design tools (like Pencil MCP) before implementing code. This allows for faster iteration on layout/design decisions without the overhead of code changes, better stakeholder alignment before committing development time, and clearer visual direction that can be referenced during implementation.
- Bulk decision validation is high-leverage: clearing 44 decisions in one session dropped unverified rate from 88% to 15%. Batch reviewing by category (architecture, design, skills, roadmap) is more efficient than one-by-one.
- Eval tracking should be Phase 0 in any governance roadmap. Build the scoreboard before building the governance - you can't improve what you don't measure.
- When building Python skills that access ArtifactStore, use store.store_fact() not store.store_artifact(). The API is method-per-type (store_fact, store_decision, store_episode), not generic.
- Critical analysis of governance roadmaps: distinguish between incident-driven improvements (needed now) vs speculative improvements (build when pain appears). The right question is "what's the incident list that motivates this?"
- code_review_verifier skill works well for Python AST analysis but has false positives: flagged TemplateCompiler.compile() as banned function because it matches the pattern for Python's built-in compile(). Need to improve detection to distinguish method calls from built-in calls.
- test_generate skill produces useful test stubs but generated tests need manual refinement: imports assume modules are available (need try/except guards), async tests need pytest-asyncio which may not be installed, mock classes may miss abstract methods from base classes.
- Pipeline validation on real project (olootu) revealed test coverage gap: 126 Python files with 427 testable units but only 1 test file. test_generate can bootstrap coverage but requires dependency installation (poetry add pytest pytest-asyncio pytest-cov) to run generated tests.
- Code pipeline execution order matters: run code_review first to identify hotspots, then test_coverage to find gaps, then test_generate for untested modules. Don't run code_refactor until tests exist to verify behavior preservation.
- When integrating skills that return results, always verify the exact key names in the return dict. The code_pipeline fix stage was checking for "changes_count" but code_refactor returns "changes" (a list). Use len(result.get("changes", [])) instead of result.get("changes_count", 0).

## Tasks Completed
- Built complete Next.js 14 Studio Web UI for Olootu BYOC Template Platform: Successfully created 64 files (11,635 lines) including: TypeScript types, API client, layout components (Navbar, Sidebar, Footer), Landing page with hero/features/CTA, Templates gallery with mock data fallback, Template detail page with dynamic form generation (VariableInput supporting text/number/select/image/color/boolean), TemplateRunner with execution flow, Providers page with status and setup guides, Settings page for API key management. Build passes successfully. Pushed to GitHub commit c9cd170.
- Started creating UI mockups for Olootu Studio using Pencil design tools: In progress - retrieved Swiss Clean style guide and design system guidelines. Ready to create mockup frames for Landing page, Templates gallery, and Template detail screens. Session ended before completing mockups.
- Set up Duro eval tracking system: Created eval_metrics skill (ops/eval_metrics.py) that computes decision review rate, unverified rate, reopen rate. Registered in skills index. Created weekly_eval_metrics checklist. Validated 3 Phase 1 Feedback Loop decisions. Current baseline: 60 real decisions, 50 unverified (after validations), 9 validated, 0% reopen rate.
- Clear decision review backlog: Validated 44 decisions in bulk (40 validated, 4 superseded). Unverified rate dropped from 88% to 15%. Only 9 decisions remain needing review. Decision feedback loop now closed.
- Session: Duro upgrade planning and eval tracking setup: 1) Critically analyzed governance roadmap - identified incident-driven vs speculative improvements. 2) Built eval_metrics skill tracking decision review rate, unverified rate, reopen rate. 3) Created weekly_eval_metrics checklist. 4) Cleared decision backlog: 44 decisions reviewed, unverified rate 88%â†’15%. 5) Stored 3 metrics snapshots as facts for trend tracking.
- Run code dev pipeline on olootu project: Successfully validated code_review_verifier and test_generate skills. Generated 110 test stubs covering 5 priority modules. Identified skill improvements needed: false positive detection in code_review, dependency handling in test_generate.
- Fix code_review_verifier false positive for method calls: Distinguished ast.Name calls (built-ins like eval) from ast.Attribute calls (methods like obj.compile). Added DANGEROUS_ATTR_CALLS set for explicit module.function patterns. Verified fix catches real threats while ignoring safe method calls.
- Fix test_generate skill for async test handling: Fixed import order (pytest before skip_async), added @skip_async and @pytest.mark.asyncio decorators to all async test generation methods, added try/except guards around module imports. Commit 83e0d9c pushed to duro repo.
- Create code_pipeline orchestrator skill: Created skill that runs code_review, test_coverage, and test_generate stages in sequence. Features: health score (0-100), unified report, recommendations, CI exit codes. Tested on olootu: 171 files, 275 findings, 8 tests generated. Commit a2e543a.
- Validate code_refactor skill on olootu: Validated 8 refactoring operations on 119 Python files. Results: remove_unused_imports (100% success, 6/20 changes), sort_imports (100% success, 15/20 changes), rename_* (working), extract_function (working). Bug found: remove_dead_code incorrectly flagged JSONResponse return content. 3 operations not yet implemented (add_type_hints, extract_variable, inline_variable).
- Fix remove_dead_code false positive bug: Rewrote remove_dead_code to use AST analysis instead of line-by-line parsing. Now correctly handles multi-line return/raise statements. Tested on 20 olootu files with 0 false positives. Commit dadcc6f.
- Validate test_coverage_verifier skill: All features validated with synthetic test data. 3 format parsers working (Cobertura XML, JSON, LCOV). Features tested: threshold enforcement, uncovered file reporting, regression detection, exclusion patterns. Skill is production-ready.
- Debug and fix --fix mode in code_pipeline showing 0 changes: Fixed bug where run_fix_stage was checking result.get("changes_count") but code_refactor returns "changes" (list). Changed to len(result.get("changes", [])). Verified: 5 files fixed with 8 changes on olootu. Committed 178ee30, pushed to origin.
- Investigate and fix --fix mode in code_pipeline: Found initial bug (changes_count vs changes), fixed it. But discovered deeper bugs in remove_unused_imports and sort_imports that corrupt multi-line imports. After multiple failed fix attempts that made things worse, disabled fix operations entirely. Incident logged: inc_20260218_032223_cgiq4t. Committed 49e69d8.
- Validate adversarial_planning skill: Promoted core -> tested. 3-phase planning with Architect/Critic/Integrator pattern, proper go/no-go calculation based on critique severity.

## Stats
- Original size: 27612 chars
- Total learnings: 43
- Tasks completed: 37
- Failures logged: 0